<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yanxianan.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本文介绍pytorch深度学习框架。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch深度学习框架：核心原理、组件、流程与实战详解">
<meta property="og:url" content="https://yanxianan.github.io/2025/09/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/pytorch/index.html">
<meta property="og:site_name" content="蛋的博客">
<meta property="og:description" content="本文介绍pytorch深度学习框架。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-09-22T12:30:00.000Z">
<meta property="article:modified_time" content="2025-09-22T12:46:39.870Z">
<meta property="article:author" content="Yxa">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yanxianan.github.io/2025/09/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/pytorch/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>PyTorch深度学习框架：核心原理、组件、流程与实战详解 | 蛋的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">蛋的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">这里是蛋的学习记录</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yanxianan.github.io/2025/09/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Yxa">
      <meta itemprop="description" content="DO SOMETHING COOL！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="蛋的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch深度学习框架：核心原理、组件、流程与实战详解
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-22 20:30:00 / 修改时间：20:46:39" itemprop="dateCreated datePublished" datetime="2025-09-22T20:30:00+08:00">2025-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
            </span>

          
            <div class="post-description">本文介绍pytorch深度学习框架。</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="title-PyTorch深度学习框架：核心原理、组件、实战与生态详解"><a href="#title-PyTorch深度学习框架：核心原理、组件、实战与生态详解" class="headerlink" title="title: PyTorch深度学习框架：核心原理、组件、实战与生态详解"></a>title: PyTorch深度学习框架：核心原理、组件、实战与生态详解</h1><p>PyTorch是由Meta（原Facebook）开源的<strong>端到端深度学习框架</strong>，自2016年首次发布以来，凭借其<strong>动态计算图的灵活性、简洁直观的API设计、强大的科研适配能力</strong>，迅速成为学术界和工业界的主流选择。它既支持快速原型开发（如几行代码验证算法想法），也能支撑大规模模型训练（如大语言模型、计算机视觉模型），尤其在科研创新、中小规模工业应用场景中占据主导地位。本文将从PyTorch的核心定位、核心概念、架构组件、实战示例到生态系统，全面解析这一框架的原理与应用，帮助读者掌握其核心用法。</p>
<h2 id="一、PyTorch的核心定位与版本演进"><a href="#一、PyTorch的核心定位与版本演进" class="headerlink" title="一、PyTorch的核心定位与版本演进"></a>一、PyTorch的核心定位与版本演进</h2><p>在深入技术细节前，先明确PyTorch的核心价值——它的设计理念是“<strong>让深度学习更简单、更灵活</strong>”，这与TensorFlow（早期侧重静态图和工业部署）形成鲜明对比，也决定了其在科研领域的优势。</p>
<h3 id="1-核心定位"><a href="#1-核心定位" class="headerlink" title="1. 核心定位"></a>1. 核心定位</h3><ul>
<li><strong>动态优先</strong>：默认采用动态计算图（Eager Execution），代码“逐行执行、即时反馈”，调试体验与Python原生代码一致（如直接<code>print</code>中间结果）；</li>
<li><strong>科研友好</strong>：支持灵活的模型结构（如条件分支、循环、动态修改网络层数），无需提前定义完整计算图，适配科研中“快速迭代算法”的需求；</li>
<li><strong>API简洁</strong>：接口设计贴近Python直觉（如张量操作与NumPy高度兼容），学习成本低，新手可快速上手；</li>
<li><strong>生态完善</strong>：围绕核心框架形成了丰富的扩展库（如TorchVision、TorchText、Hugging Face Transformers），覆盖计算机视觉、自然语言处理、强化学习等领域。</li>
</ul>
<h3 id="2-关键版本演进"><a href="#2-关键版本演进" class="headerlink" title="2. 关键版本演进"></a>2. 关键版本演进</h3><p>PyTorch的版本迭代围绕“提升性能、完善部署能力、扩展生态”展开，核心里程碑版本如下：</p>
<table>
<thead>
<tr>
<th>版本</th>
<th>发布时间</th>
<th>核心特性</th>
</tr>
</thead>
<tbody><tr>
<td>PyTorch 0.1</td>
<td>2016年</td>
<td>首次发布，奠定动态计算图核心，支持基本张量操作和神经网络模块</td>
</tr>
<tr>
<td>PyTorch 1.0</td>
<td>2018年</td>
<td>引入TorchScript（动态图转静态图，支持部署）、分布式训练（torch.distributed）</td>
</tr>
<tr>
<td>PyTorch 1.5</td>
<td>2020年</td>
<td>增强自动微分稳定性、优化CUDA支持，引入TorchServe（模型服务化部署）</td>
</tr>
<tr>
<td>PyTorch 1.10+</td>
<td>2021-2023年</td>
<td>完善大语言模型（LLM）训练支持、优化Transformer性能、增强移动部署（TorchMobile）</td>
</tr>
<tr>
<td>PyTorch 2.0</td>
<td>2023年</td>
<td>引入Compile（即时编译优化，提升静态图性能）、改进分布式训练效率，性能接近TensorFlow</td>
</tr>
</tbody></table>
<p>目前PyTorch 2.x已成为主流，其核心改进（如<code>torch.compile</code>）在保留动态图灵活性的同时，大幅提升了训练和推理性能，缩小了与静态图框架的性能差距。</p>
<h2 id="二、PyTorch的核心概念"><a href="#二、PyTorch的核心概念" class="headerlink" title="二、PyTorch的核心概念"></a>二、PyTorch的核心概念</h2><p>PyTorch的核心概念围绕“<strong>张量（Tensor）</strong>”和“<strong>动态计算图（Dynamic Computation Graph）</strong>”展开，理解这两个概念是使用PyTorch的基础。</p>
<h3 id="1-张量（Tensor）：数据的载体"><a href="#1-张量（Tensor）：数据的载体" class="headerlink" title="1. 张量（Tensor）：数据的载体"></a>1. 张量（Tensor）：数据的载体</h3><p>张量是PyTorch中最基本的数据结构，本质是“<strong>多维数组</strong>”，用于存储输入数据、模型参数、中间计算结果。它与NumPy数组类似，但支持GPU加速、自动微分追踪，是PyTorch所有运算的基础。</p>
<h4 id="（1）张量的核心属性"><a href="#（1）张量的核心属性" class="headerlink" title="（1）张量的核心属性"></a>（1）张量的核心属性</h4><ul>
<li><strong>阶（Rank）</strong>：张量的维度数，对应NumPy的“ndim”。例如：<ul>
<li>0阶张量（标量）：<code>torch.tensor(5)</code> → 形状<code>()</code>；</li>
<li>1阶张量（向量）：<code>torch.tensor([1,2,3])</code> → 形状<code>(3,)</code>；</li>
<li>2阶张量（矩阵）：<code>torch.tensor([[1,2],[3,4]])</code> → 形状<code>(2,2)</code>；</li>
<li>高阶张量：如3阶张量（<code>(2,3,4)</code>）可表示“2个3行4列的矩阵”，4阶张量（<code>(batch_size, channels, height, width)</code>）是计算机视觉中图像数据的标准格式（如<code>(32, 3, 224, 224)</code>表示32张3通道224x224的RGB图像）。</li>
</ul>
</li>
<li><strong>形状（Shape）</strong>：各维度的元素个数，可通过<code>tensor.shape</code>或<code>tensor.size()</code>获取。</li>
<li><strong>数据类型（Dtype）</strong>：张量元素的类型，常用类型包括<code>torch.float32</code>（默认浮点类型）、<code>torch.int32</code>、<code>torch.bool</code>，需注意与NumPy类型的兼容（可通过<code>torch.as_tensor()</code>或<code>numpy()</code>转换）。</li>
<li><strong>设备（Device）</strong>：张量存储的硬件位置，分为<code>cpu</code>和<code>cuda</code>（GPU），可通过<code>tensor.device</code>获取，通过<code>tensor.to(device)</code>迁移。</li>
</ul>
<h4 id="（2）张量的创建与操作"><a href="#（2）张量的创建与操作" class="headerlink" title="（2）张量的创建与操作"></a>（2）张量的创建与操作</h4><p>PyTorch提供了丰富的张量创建和运算API，与NumPy接口高度一致，降低学习成本。示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 张量创建</span></span><br><span class="line"><span class="comment"># 从Python列表创建</span></span><br><span class="line">t1 = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 从NumPy数组创建（共享内存，修改一个会影响另一个）</span></span><br><span class="line">np_arr = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=np.float32)</span><br><span class="line">t2 = torch.as_tensor(np_arr)</span><br><span class="line"><span class="comment"># 创建全零/全一张量</span></span><br><span class="line">t3 = torch.zeros((<span class="number">2</span>, <span class="number">3</span>))  <span class="comment"># 形状(2,3)的全零张量</span></span><br><span class="line">t4 = torch.ones((<span class="number">3</span>, <span class="number">3</span>), dtype=torch.int32)  <span class="comment"># 形状(3,3)的全一张量</span></span><br><span class="line"><span class="comment"># 创建随机张量（正态分布）</span></span><br><span class="line">t5 = torch.randn((<span class="number">2</span>, <span class="number">2</span>))  <span class="comment"># 均值0、方差1的正态分布</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 基本运算（支持广播机制）</span></span><br><span class="line">t_add = t1 + t2  <span class="comment"># 元素加法（等价于torch.add(t1, t2)）</span></span><br><span class="line">t_mul = t1 * t2  <span class="comment"># 元素乘法（等价于torch.mul(t1, t2)）</span></span><br><span class="line">t_matmul = torch.matmul(t1, t2)  <span class="comment"># 矩阵乘法（等价于t1 @ t2）</span></span><br><span class="line">t_reshape = t1.view((<span class="number">1</span>, <span class="number">4</span>))  <span class="comment"># 形状重塑（从(2,2)到(1,4)，需保证元素总数一致）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 设备迁移（CPU ↔ GPU）</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">t1_cuda = t1.to(device)  <span class="comment"># 迁移到GPU（若有GPU）</span></span><br><span class="line">t1_cpu = t1_cuda.cpu()   <span class="comment"># 迁移回CPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 与NumPy转换</span></span><br><span class="line">t1_np = t1.numpy()  <span class="comment"># 张量→NumPy数组（CPU张量才可转换）</span></span><br><span class="line">np_to_t = torch.from_numpy(np_arr)  <span class="comment"># NumPy数组→张量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵乘法结果:\n&quot;</span>, t_matmul)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># 矩阵乘法结果:</span></span><br><span class="line"><span class="comment">#  tensor([[19., 22.],</span></span><br><span class="line"><span class="comment">#         [43., 50.]])</span></span><br></pre></td></tr></table></figure>

<h4 id="（3）可训练张量：requires-grad与梯度追踪"><a href="#（3）可训练张量：requires-grad与梯度追踪" class="headerlink" title="（3）可训练张量：requires_grad与梯度追踪"></a>（3）可训练张量：requires_grad与梯度追踪</h4><p>PyTorch的张量通过<code>requires_grad=True</code>标记为“<strong>可训练张量</strong>”（通常是模型的权重和偏置），自动微分机制会追踪其参与的所有运算，以便后续计算梯度。示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建可训练张量（requires_grad=True）</span></span><br><span class="line">w = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># 权重</span></span><br><span class="line">b = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># 偏置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义运算（y = w*x + b）</span></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">y_pred = w * x + b</span><br><span class="line">y_true = torch.tensor([<span class="number">3.0</span>, <span class="number">5.0</span>, <span class="number">7.0</span>])  <span class="comment"># 真实标签（y=2x+1）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失（均方误差）</span></span><br><span class="line">loss = torch.mean((y_pred - y_true) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播计算梯度（对w和b求导）</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看梯度（d(loss)/dw 和 d(loss)/db）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;w的梯度:&quot;</span>, w.grad)  <span class="comment"># 输出0.0（因参数刚好匹配，损失为0）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b的梯度:&quot;</span>, b.grad)  <span class="comment"># 输出0.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;是否追踪梯度:&quot;</span>, w.requires_grad)  <span class="comment"># 输出True</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>关键说明</strong>：只有<code>requires_grad=True</code>的张量才会被追踪梯度；运算结果（如<code>y_pred</code>、<code>loss</code>）会自动继承<code>requires_grad</code>属性；<code>backward()</code>后，梯度会存储在<code>tensor.grad</code>中，下次反向传播前需用<code>optimizer.zero_grad()</code>或<code>tensor.grad.zero_()</code>清空梯度，避免累积。</li>
</ul>
<h3 id="2-动态计算图（Dynamic-Computation-Graph）：灵活的运算流程"><a href="#2-动态计算图（Dynamic-Computation-Graph）：灵活的运算流程" class="headerlink" title="2. 动态计算图（Dynamic Computation Graph）：灵活的运算流程"></a>2. 动态计算图（Dynamic Computation Graph）：灵活的运算流程</h3><p>计算图是描述“张量运算依赖关系”的抽象结构（节点&#x3D;运算，边&#x3D;张量流向），PyTorch的核心优势是“<strong>动态计算图</strong>”——运算与图构建同步进行，支持实时修改图结构（如条件、循环），调试更直观。</p>
<h4 id="动态图vs静态图（以TensorFlow-1-x为例）"><a href="#动态图vs静态图（以TensorFlow-1-x为例）" class="headerlink" title="动态图vs静态图（以TensorFlow 1.x为例）"></a>动态图vs静态图（以TensorFlow 1.x为例）</h4><table>
<thead>
<tr>
<th>特性</th>
<th>PyTorch动态图</th>
<th>TensorFlow 1.x静态图</th>
</tr>
</thead>
<tbody><tr>
<td>图构建时机</td>
<td>运算执行时实时构建</td>
<td>先定义完整图，再通过Session执行</td>
</tr>
<tr>
<td>灵活性</td>
<td>支持条件（if）、循环（for）、动态修改网络</td>
<td>需通过控制流算子（如tf.cond），灵活性低</td>
</tr>
<tr>
<td>调试体验</td>
<td>可直接print中间结果，报错定位清晰</td>
<td>需通过Session.run()获取结果，调试复杂</td>
</tr>
<tr>
<td>适用场景</td>
<td>科研快速迭代、复杂动态模型</td>
<td>大规模工业部署、固定流程模型</td>
</tr>
</tbody></table>
<h4 id="动态图示例：带条件分支的模型"><a href="#动态图示例：带条件分支的模型" class="headerlink" title="动态图示例：带条件分支的模型"></a>动态图示例：带条件分支的模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dynamic_model</span>(<span class="params">x, use_relu</span>):</span><br><span class="line">    <span class="comment"># 动态条件分支：根据use_relu决定是否使用ReLU激活</span></span><br><span class="line">    <span class="keyword">if</span> use_relu:</span><br><span class="line">        y = torch.relu(x ** <span class="number">2</span> - <span class="number">4</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = x ** <span class="number">2</span> - <span class="number">4</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试两种分支（动态构建不同计算图）</span></span><br><span class="line">x = torch.tensor([-<span class="number">3.0</span>, -<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分支1：使用ReLU</span></span><br><span class="line">y1 = dynamic_model(x, use_relu=<span class="literal">True</span>)</span><br><span class="line">y1.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;ReLU分支x的梯度:&quot;</span>, x.grad)  <span class="comment"># 梯度仅非负部分有值</span></span><br><span class="line">x.grad.zero_()  <span class="comment"># 清空梯度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分支2：不使用ReLU</span></span><br><span class="line">y2 = dynamic_model(x, use_relu=<span class="literal">False</span>)</span><br><span class="line">y2.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;无ReLU分支x的梯度:&quot;</span>, x.grad)  <span class="comment"># 梯度全量计算</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>核心优势</strong>：上述代码中，<code>use_relu</code>为<code>True</code>或<code>False</code>时，计算图结构完全不同，PyTorch可实时适配，无需提前定义两种图结构——这对科研中“动态调整模型逻辑”（如根据输入长度修改网络层数）至关重要。</li>
</ul>
<h3 id="3-自动微分（Autograd）：梯度计算的核心"><a href="#3-自动微分（Autograd）：梯度计算的核心" class="headerlink" title="3. 自动微分（Autograd）：梯度计算的核心"></a>3. 自动微分（Autograd）：梯度计算的核心</h3><p>PyTorch的<code>autograd</code>模块是动态计算图的“梯度引擎”，通过<strong>反向模式自动微分</strong>（Reverse Mode Automatic Differentiation）计算张量的梯度，无需手动推导导数公式。其核心逻辑是：</p>
<ol>
<li><strong>前向传播</strong>：执行张量运算时，<code>autograd</code>记录运算操作和依赖关系（存储在<code>Function</code>对象中）；</li>
<li><strong>反向传播</strong>：调用<code>loss.backward()</code>时，从损失张量（标量）出发，沿依赖关系反向遍历计算图，通过链式法则计算每个<code>requires_grad=True</code>张量的梯度；</li>
<li><strong>梯度存储</strong>：梯度结果存储在张量的<code>grad</code>属性中，供优化器更新参数。</li>
</ol>
<h4 id="关键特性："><a href="#关键特性：" class="headerlink" title="关键特性："></a>关键特性：</h4><ul>
<li><strong>支持非标量梯度</strong>：若输出是向量&#x2F;矩阵，需通过<code>backward()</code>的<code>grad_tensors</code>参数指定“梯度权重”（如<code>y.backward(torch.ones_like(y))</code>）；</li>
<li><strong>梯度切断</strong>：通过<code>torch.no_grad()</code>或<code>tensor.detach()</code>临时禁用梯度追踪（如验证阶段冻结模型参数）；</li>
<li><strong>自定义梯度</strong>：通过<code>torch.autograd.Function</code>自定义运算的前向和反向传播逻辑（如实现自定义算子）。</li>
</ul>
<h2 id="三、PyTorch的核心组件（架构与生态）"><a href="#三、PyTorch的核心组件（架构与生态）" class="headerlink" title="三、PyTorch的核心组件（架构与生态）"></a>三、PyTorch的核心组件（架构与生态）</h2><p>PyTorch的核心组件围绕“模型构建→数据处理→训练优化→可视化→部署”全流程设计，各组件简洁直观，且无缝协作。</p>
<h3 id="1-torch-nn：神经网络构建核心"><a href="#1-torch-nn：神经网络构建核心" class="headerlink" title="1. torch.nn：神经网络构建核心"></a>1. torch.nn：神经网络构建核心</h3><p><code>torch.nn</code>是PyTorch用于构建神经网络的核心模块，提供了“<strong>层（Layers）</strong>”、“<strong>损失函数（Loss Functions）</strong>”、“<strong>模型容器（Containers）</strong>”等基础组件，支持快速搭建各类深度学习模型（如CNN、RNN、Transformer）。</p>
<h4 id="（1）核心容器：Sequential与Module"><a href="#（1）核心容器：Sequential与Module" class="headerlink" title="（1）核心容器：Sequential与Module"></a>（1）核心容器：Sequential与Module</h4><ul>
<li><strong>Sequential</strong>：适用于“线性堆叠的网络”（无分支、无跨层连接），如简单全连接网络、基础CNN，使用简单但灵活性低；</li>
<li><strong>Module</strong>：自定义模型的基类，适用于“复杂网络”（有分支、跨层连接、动态逻辑），需重写<code>__init__</code>（定义层）和<code>forward</code>（定义前向传播逻辑），灵活性最高。</li>
</ul>
<h4 id="示例1：用Sequential构建全连接网络（MNIST分类）"><a href="#示例1：用Sequential构建全连接网络（MNIST分类）" class="headerlink" title="示例1：用Sequential构建全连接网络（MNIST分类）"></a>示例1：用Sequential构建全连接网络（MNIST分类）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建线性堆叠网络</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Flatten(input_size=(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)),  <span class="comment"># 输入层：展平28x28单通道图像→784维向量</span></span><br><span class="line">    nn.Linear(<span class="number">784</span>, <span class="number">128</span>),                 <span class="comment"># 隐藏层：784→128维</span></span><br><span class="line">    nn.ReLU(),                           <span class="comment"># ReLU激活函数</span></span><br><span class="line">    nn.Dropout(<span class="number">0.2</span>),                     <span class="comment"># Dropout层：防止过拟合，随机丢弃20%神经元</span></span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">10</span>),                  <span class="comment"># 输出层：128→10维（MNIST 0-9分类）</span></span><br><span class="line">    nn.Softmax(dim=<span class="number">1</span>)                    <span class="comment"># Softmax激活：输出类别概率</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看模型结构</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># Sequential(</span></span><br><span class="line"><span class="comment">#   (0): Flatten(start_dim=1, end_dim=-1)</span></span><br><span class="line"><span class="comment">#   (1): Linear(in_features=784, out_features=128, bias=True)</span></span><br><span class="line"><span class="comment">#   (2): ReLU()</span></span><br><span class="line"><span class="comment">#   (3): Dropout(p=0.2, inplace=False)</span></span><br><span class="line"><span class="comment">#   (4): Linear(in_features=128, out_features=10, bias=True)</span></span><br><span class="line"><span class="comment">#   (5): Softmax(dim=1)</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模型输入输出</span></span><br><span class="line">x = torch.randn(<span class="number">32</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)  <span class="comment"># 32个样本，1通道，28x28图像</span></span><br><span class="line">y_pred = model(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出形状:&quot;</span>, y_pred.shape)  <span class="comment"># 输出(32, 10)：32个样本，每个样本10个类别概率</span></span><br></pre></td></tr></table></figure>

<h4 id="示例2：用Module自定义残差块（ResNet核心）"><a href="#示例2：用Module自定义残差块（ResNet核心）" class="headerlink" title="示例2：用Module自定义残差块（ResNet核心）"></a>示例2：用Module自定义残差块（ResNet核心）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义残差块（有跨层连接）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(ResidualBlock, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 主路径：两个3x3卷积+ReLU</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#  shortcut路径：若输入输出通道数不一致，用1x1卷积调整</span></span><br><span class="line">        <span class="variable language_">self</span>.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>) <span class="keyword">if</span> in_channels != out_channels <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 前向传播逻辑</span></span><br><span class="line">        residual = <span class="variable language_">self</span>.shortcut(x)  <span class="comment"># shortcut路径</span></span><br><span class="line">        out = F.relu(<span class="variable language_">self</span>.conv1(x))  <span class="comment"># 主路径第一卷积+ReLU</span></span><br><span class="line">        out = <span class="variable language_">self</span>.conv2(out)        <span class="comment"># 主路径第二卷积</span></span><br><span class="line">        out += residual              <span class="comment"># 主路径+shortcut路径（残差连接）</span></span><br><span class="line">        out = F.relu(out)            <span class="comment"># 最终ReLU</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建ResNet-like模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleResNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>)  <span class="comment"># 输入3通道（RGB）</span></span><br><span class="line">        <span class="variable language_">self</span>.pool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.block1 = ResidualBlock(<span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.block2 = ResidualBlock(<span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.block3 = ResidualBlock(<span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.avg_pool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># 自适应全局平均池化→(1,1)</span></span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">64</span>, num_classes)          <span class="comment"># 全连接层：64→1000类（ImageNet）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.pool(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.block1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.block2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.block3(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.avg_pool(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)  <span class="comment"># 展平：(batch_size, 64, 1, 1)→(batch_size, 64)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line">model = SimpleResNet(num_classes=<span class="number">1000</span>)</span><br><span class="line">x = torch.randn(<span class="number">32</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)  <span class="comment"># 32个RGB图像，224x224</span></span><br><span class="line">y_pred = model(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型输出形状:&quot;</span>, y_pred.shape)  <span class="comment"># 输出(32, 1000)</span></span><br></pre></td></tr></table></figure>

<h4 id="（2）常用层与损失函数"><a href="#（2）常用层与损失函数" class="headerlink" title="（2）常用层与损失函数"></a>（2）常用层与损失函数</h4><ul>
<li><strong>卷积层</strong>：<code>nn.Conv2d</code>（2D卷积，用于图像）、<code>nn.Conv1d</code>（1D卷积，用于文本）；</li>
<li><strong>池化层</strong>：<code>nn.MaxPool2d</code>（最大池化）、<code>nn.AvgPool2d</code>（平均池化）；</li>
<li><strong>循环层</strong>：<code>nn.RNN</code>、<code>nn.LSTM</code>、<code>nn.GRU</code>（用于序列数据，如文本、时序）；</li>
<li><strong>Transformer层</strong>：<code>nn.Transformer</code>、<code>nn.TransformerEncoder</code>、<code>nn.TransformerDecoder</code>（用于大语言模型、机器翻译）；</li>
<li><strong>损失函数</strong>：<code>nn.MSELoss</code>（均方误差，用于回归）、<code>nn.CrossEntropyLoss</code>（交叉熵，用于分类）、<code>nn.BCELoss</code>（二分类交叉熵）。</li>
</ul>
<h3 id="2-torch-optim：优化器与参数更新"><a href="#2-torch-optim：优化器与参数更新" class="headerlink" title="2. torch.optim：优化器与参数更新"></a>2. torch.optim：优化器与参数更新</h3><p><code>torch.optim</code>提供了主流的<strong>优化算法</strong>，用于根据<code>autograd</code>计算的梯度更新模型参数（如权重<code>w</code>、偏置<code>b</code>）。核心逻辑是：</p>
<ol>
<li>初始化优化器，传入待更新的模型参数和超参数（如学习率<code>lr</code>）；</li>
<li>每次训练迭代前，调用<code>optimizer.zero_grad()</code>清空梯度（避免梯度累积）；</li>
<li>前向传播计算损失；</li>
<li>反向传播计算梯度（<code>loss.backward()</code>）；</li>
<li>调用<code>optimizer.step()</code>更新参数。</li>
</ol>
<h4 id="示例：用Adam优化器训练线性模型"><a href="#示例：用Adam优化器训练线性模型" class="headerlink" title="示例：用Adam优化器训练线性模型"></a>示例：用Adam优化器训练线性模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义模型（简单线性回归：y = w*x + b）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 输入1维，输出1维</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 初始化模型、损失函数、优化器</span></span><br><span class="line">model = LinearModel()</span><br><span class="line">criterion = nn.MSELoss()  <span class="comment"># 均方误差损失</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># Adam优化器，学习率0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 模拟训练数据（y = 2x + 3 + 噪声）</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)  <span class="comment"># 100个样本，1维输入</span></span><br><span class="line">y_true = <span class="number">2</span> * x + <span class="number">3</span> + <span class="number">0.1</span> * torch.randn(<span class="number">100</span>, <span class="number">1</span>)  <span class="comment"># 带噪声的真实标签</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 训练循环（100轮）</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = criterion(y_pred, y_true)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 每10轮打印损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看训练后的参数（接近真实值w=2，b=3）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练后的权重:&quot;</span>, model.linear.weight.data.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练后的偏置:&quot;</span>, model.linear.bias.data.item())</span><br></pre></td></tr></table></figure>

<h4 id="常用优化器："><a href="#常用优化器：" class="headerlink" title="常用优化器："></a>常用优化器：</h4><ul>
<li><strong>SGD</strong>：随机梯度下降，基础优化器，需配合动量（<code>momentum</code>）提升性能；</li>
<li><strong>Adam</strong>：自适应学习率优化器，收敛快，适用于大多数场景；</li>
<li><strong>RMSprop</strong>：自适应学习率，适合非平稳目标（如RNN训练）；</li>
<li><strong>AdamW</strong>：Adam的改进版，加入权重衰减（Weight Decay），缓解过拟合。</li>
</ul>
<h3 id="3-torch-utils-data：高效数据输入流水线"><a href="#3-torch-utils-data：高效数据输入流水线" class="headerlink" title="3. torch.utils.data：高效数据输入流水线"></a>3. torch.utils.data：高效数据输入流水线</h3><p>深度学习训练需处理海量数据（如百万级图像、TB级文本），<code>torch.utils.data</code>提供了“<strong>数据集抽象</strong>”和“<strong>批量加载</strong>”功能，支持：</p>
<ul>
<li>自定义数据集（通过<code>Dataset</code>基类）；</li>
<li>批量加载（<code>DataLoader</code>）；</li>
<li>数据打乱（<code>shuffle</code>）、并行预处理（<code>num_workers</code>）；</li>
<li>数据划分（<code>random_split</code>）。</li>
</ul>
<h4 id="示例1：自定义图像数据集（加载本地图像）"><a href="#示例1：自定义图像数据集（加载本地图像）" class="headerlink" title="示例1：自定义图像数据集（加载本地图像）"></a>示例1：自定义图像数据集（加载本地图像）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集（继承Dataset）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomImageDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_dir, label_file, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            img_dir: 图像文件夹路径</span></span><br><span class="line"><span class="string">            label_file: 标签文件路径（每行格式：图像名 标签）</span></span><br><span class="line"><span class="string">            transform: 图像预处理函数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.img_dir = img_dir</span><br><span class="line">        <span class="variable language_">self</span>.transform = transform</span><br><span class="line">        <span class="comment"># 读取标签文件</span></span><br><span class="line">        <span class="variable language_">self</span>.labels = &#123;&#125;</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(label_file, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                img_name, label = line.strip().split()</span><br><span class="line">                <span class="variable language_">self</span>.labels[img_name] = <span class="built_in">int</span>(label)</span><br><span class="line">        <span class="variable language_">self</span>.img_names = <span class="built_in">list</span>(<span class="variable language_">self</span>.labels.keys())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回数据集总样本数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.img_names)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 根据索引返回单个样本（图像+标签）</span></span><br><span class="line">        img_name = <span class="variable language_">self</span>.img_names[idx]</span><br><span class="line">        img_path = os.path.join(<span class="variable language_">self</span>.img_dir, img_name)</span><br><span class="line">        image = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&#x27;RGB&#x27;</span>)  <span class="comment"># 读取图像并转为RGB</span></span><br><span class="line">        label = <span class="variable language_">self</span>.labels[img_name]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 图像预处理（如 resize、归一化）</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.transform:</span><br><span class="line">            image = <span class="variable language_">self</span>.transform(image)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义图像预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 缩放为224x224</span></span><br><span class="line">    transforms.ToTensor(),          <span class="comment"># 转为张量（0-1归一化）</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 标准化（ImageNet均值/标准差）</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化数据集</span></span><br><span class="line">dataset = CustomImageDataset(</span><br><span class="line">    img_dir=<span class="string">&quot;path/to/images&quot;</span>,</span><br><span class="line">    label_file=<span class="string">&quot;path/to/labels.txt&quot;</span>,</span><br><span class="line">    transform=transform</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和验证集（8:2）</span></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(dataset))</span><br><span class="line">val_size = <span class="built_in">len</span>(dataset) - train_size</span><br><span class="line">train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化DataLoader（批量加载）</span></span><br><span class="line">train_loader = DataLoader(</span><br><span class="line">    train_dataset,</span><br><span class="line">    batch_size=<span class="number">32</span>,  <span class="comment"># 批次大小</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,   <span class="comment"># 训练集打乱</span></span><br><span class="line">    num_workers=<span class="number">4</span>   <span class="comment"># 4个进程并行加载数据（缓解CPU瓶颈）</span></span><br><span class="line">)</span><br><span class="line">val_loader = DataLoader(</span><br><span class="line">    val_dataset,</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    shuffle=<span class="literal">False</span>,  <span class="comment"># 验证集不打乱</span></span><br><span class="line">    num_workers=<span class="number">4</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历DataLoader（训练时使用）</span></span><br><span class="line"><span class="keyword">for</span> batch_idx, (images, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Batch <span class="subst">&#123;batch_idx+<span class="number">1</span>&#125;</span>, Images shape: <span class="subst">&#123;images.shape&#125;</span>, Labels shape: <span class="subst">&#123;labels.shape&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 输出：Batch 1, Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])</span></span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h4 id="示例2：使用内置数据集（TorchVision提供）"><a href="#示例2：使用内置数据集（TorchVision提供）" class="headerlink" title="示例2：使用内置数据集（TorchVision提供）"></a>示例2：使用内置数据集（TorchVision提供）</h4><p>PyTorch的<code>torchvision.datasets</code>提供了常用公开数据集（如MNIST、CIFAR10、ImageNet），可直接调用，无需手动处理文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载MNIST数据集（自动下载到./data目录）</span></span><br><span class="line">train_dataset = MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,  <span class="comment"># 训练集</span></span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=transforms.ToTensor()  <span class="comment"># 转为张量</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化DataLoader</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>


<h3 id="4-可视化工具：TensorBoard与Weights-Biases"><a href="#4-可视化工具：TensorBoard与Weights-Biases" class="headerlink" title="4. 可视化工具：TensorBoard与Weights &amp; Biases"></a>4. 可视化工具：TensorBoard与Weights &amp; Biases</h3><p>PyTorch支持与主流可视化工具集成，实时监控训练过程（如损失、准确率、梯度分布），帮助分析模型训练情况。</p>
<h4 id="（1）TensorBoard"><a href="#（1）TensorBoard" class="headerlink" title="（1）TensorBoard"></a>（1）TensorBoard</h4><p>PyTorch通过<code>torch.utils.tensorboard</code>集成TensorBoard，需先安装TensorBoard（<code>pip install tensorboard</code>）。示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 初始化SummaryWriter（日志保存路径）</span></span><br><span class="line">writer = SummaryWriter(log_dir=<span class="string">&quot;runs/mnist_experiment&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 加载数据和模型（复用之前的线性模型）</span></span><br><span class="line">train_dataset = MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">model = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>), nn.Softmax(dim=<span class="number">1</span>))</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 训练并记录日志</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, (images, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="comment"># 每100批次记录一次损失</span></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            avg_loss = running_loss / <span class="number">100</span></span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;Training Loss&#x27;</span>, avg_loss, epoch * <span class="built_in">len</span>(train_loader) + i)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 记录每轮的模型参数分布（如第一层权重）</span></span><br><span class="line">    writer.add_histogram(<span class="string">&#x27;Layer 1 Weight&#x27;</span>, model[<span class="number">1</span>].weight.data, epoch)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 关闭Writer</span></span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动TensorBoard（终端执行）</span></span><br><span class="line"><span class="comment"># tensorboard --logdir=runs</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>可视化内容</strong>：在浏览器访问<code>http://localhost:6006</code>，可查看“Training Loss”曲线（损失变化）、“Layer 1 Weight”直方图（参数分布）。</li>
</ul>
<h4 id="（2）Weights-Biases（W-B）"><a href="#（2）Weights-Biases（W-B）" class="headerlink" title="（2）Weights &amp; Biases（W&amp;B）"></a>（2）Weights &amp; Biases（W&amp;B）</h4><p>W&amp;B是更强大的实验管理工具，支持多实验对比、模型版本控制、团队协作，PyTorch集成简单（<code>pip install wandb</code>），适合科研和团队开发。</p>
<h3 id="5-分布式训练：torch-distributed"><a href="#5-分布式训练：torch-distributed" class="headerlink" title="5. 分布式训练：torch.distributed"></a>5. 分布式训练：torch.distributed</h3><p>当模型规模大（如大语言模型）或数据量多时，单机单GPU训练速度慢，PyTorch通过<code>torch.distributed</code>支持<strong>多GPU、多机器训练</strong>，核心策略包括：</p>
<ul>
<li><strong>DataParallel（DP）</strong>：单机多GPU，简单易用但性能一般（仅主GPU计算梯度，再广播到其他GPU）；</li>
<li><strong>DistributedDataParallel（DDP）</strong>：单机&#x2F;多机多GPU，性能更优（各GPU独立计算梯度，再同步梯度），是主流选择；</li>
<li><strong>RPC</strong>：远程过程调用，支持分布式模型（如模型并行）。</li>
</ul>
<h4 id="示例：用DDP实现单机多GPU训练"><a href="#示例：用DDP实现单机多GPU训练" class="headerlink" title="示例：用DDP实现单机多GPU训练"></a>示例：用DDP实现单机多GPU训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, DistributedSampler</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化分布式环境</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_distributed</span>(<span class="params">rank, world_size</span>):</span><br><span class="line">    os.environ[<span class="string">&#x27;MASTER_ADDR&#x27;</span>] = <span class="string">&#x27;localhost&#x27;</span>  <span class="comment"># 主节点地址</span></span><br><span class="line">    os.environ[<span class="string">&#x27;MASTER_PORT&#x27;</span>] = <span class="string">&#x27;12355&#x27;</span>       <span class="comment"># 主节点端口</span></span><br><span class="line">    dist.init_process_group(backend=<span class="string">&#x27;nccl&#x27;</span>, rank=rank, world_size=world_size)  <span class="comment"># NCCL是GPU间通信后端</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">rank, world_size</span>):</span><br><span class="line">    <span class="comment"># 初始化分布式环境</span></span><br><span class="line">    init_distributed(rank, world_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置当前GPU</span></span><br><span class="line">    torch.cuda.set_device(rank)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载数据（使用DistributedSampler分片数据）</span></span><br><span class="line">    transform = transforms.ToTensor()</span><br><span class="line">    dataset = MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)  <span class="comment"># 数据分片</span></span><br><span class="line">    dataloader = DataLoader(dataset, batch_size=<span class="number">32</span>, sampler=sampler)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化模型并迁移到GPU</span></span><br><span class="line">    model = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>), nn.Softmax(dim=<span class="number">1</span>)).cuda(rank)</span><br><span class="line">    model = DDP(model, device_ids=[rank])  <span class="comment"># 包装为DDP模型</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 损失函数和优化器</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练循环</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        sampler.set_epoch(epoch)  <span class="comment"># 每轮打乱数据分片</span></span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> images, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">            images, labels = images.cuda(rank), labels.cuda(rank)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            running_loss += loss.item()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Rank <span class="subst">&#123;rank&#125;</span>, Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;running_loss/<span class="built_in">len</span>(dataloader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 销毁分布式环境</span></span><br><span class="line">    dist.destroy_process_group()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    world_size = torch.cuda.device_count()  <span class="comment"># 可用GPU数量</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Using <span class="subst">&#123;world_size&#125;</span> GPUs&quot;</span>)</span><br><span class="line">    mp.spawn(train, args=(world_size,), nprocs=world_size, join=<span class="literal">True</span>)  <span class="comment"># 启动多进程</span></span><br></pre></td></tr></table></figure>


<h3 id="6-模型部署：从科研到产品"><a href="#6-模型部署：从科研到产品" class="headerlink" title="6. 模型部署：从科研到产品"></a>6. 模型部署：从科研到产品</h3><p>PyTorch早期的部署能力弱于TensorFlow，但随着版本迭代，已形成完善的部署生态，支持多平台部署：</p>
<h4 id="（1）TorchScript：动态图转静态图"><a href="#（1）TorchScript：动态图转静态图" class="headerlink" title="（1）TorchScript：动态图转静态图"></a>（1）TorchScript：动态图转静态图</h4><p>TorchScript将PyTorch动态图模型转换为“<strong>静态图模型</strong>”（TorchScript Module），支持脱离Python环境运行（如C++部署），核心步骤：</p>
<ol>
<li>用<code>torch.jit.trace</code>（追踪式）或<code>torch.jit.script</code>（脚本式）转换模型；</li>
<li>保存转换后的模型（<code>.pt</code>或<code>.pth</code>）；</li>
<li>在C++中用LibTorch加载模型并推理。</li>
</ol>
<h4 id="示例：TorchScript转换与保存"><a href="#示例：TorchScript转换与保存" class="headerlink" title="示例：TorchScript转换与保存"></a>示例：TorchScript转换与保存</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.relu(<span class="variable language_">self</span>.linear(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化并训练模型（省略训练步骤）</span></span><br><span class="line">model = SimpleModel()</span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 切换到评估模式（禁用Dropout、BatchNorm等）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 追踪式转换（需提供示例输入）</span></span><br><span class="line">example_input = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">traced_model = torch.jit.trace(model, example_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 脚本式转换（支持动态逻辑，无需示例输入）</span></span><br><span class="line">scripted_model = torch.jit.script(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 保存模型</span></span><br><span class="line">traced_model.save(<span class="string">&quot;traced_model.pt&quot;</span>)</span><br><span class="line">scripted_model.save(<span class="string">&quot;scripted_model.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 加载模型（Python或C++）</span></span><br><span class="line">loaded_model = torch.jit.load(<span class="string">&quot;traced_model.pt&quot;</span>)</span><br><span class="line">output = loaded_model(example_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;加载模型输出:&quot;</span>, output.shape)  <span class="comment"># 输出(1, 2)</span></span><br></pre></td></tr></table></figure>

<h4 id="（2）ONNX：跨框架转换"><a href="#（2）ONNX：跨框架转换" class="headerlink" title="（2）ONNX：跨框架转换"></a>（2）ONNX：跨框架转换</h4><p>ONNX（Open Neural Network Exchange）是跨框架模型格式，支持将PyTorch模型转换为ONNX格式，再部署到TensorRT（NVIDIA GPU加速）、TensorFlow Lite（移动设备）等框架。</p>
<h4 id="（3）TorchServe：模型服务化部署"><a href="#（3）TorchServe：模型服务化部署" class="headerlink" title="（3）TorchServe：模型服务化部署"></a>（3）TorchServe：模型服务化部署</h4><p>TorchServe是PyTorch官方的“<strong>模型服务框架</strong>”，支持快速部署模型为HTTP&#x2F;HTTPS服务，提供负载均衡、模型版本控制、A&#x2F;B测试等功能，适合云服务部署。</p>
<h4 id="（4）TorchMobile：移动与嵌入式部署"><a href="#（4）TorchMobile：移动与嵌入式部署" class="headerlink" title="（4）TorchMobile：移动与嵌入式部署"></a>（4）TorchMobile：移动与嵌入式部署</h4><p>TorchMobile是PyTorch的移动部署库，支持将模型优化后部署到Android、iOS设备，通过量化（Quantization）、剪枝（Pruning）减小模型体积、提升推理速度。</p>
<h2 id="四、PyTorch的典型应用场景"><a href="#四、PyTorch的典型应用场景" class="headerlink" title="四、PyTorch的典型应用场景"></a>四、PyTorch的典型应用场景</h2><p>PyTorch的灵活性和生态完善性使其适用于各类深度学习场景，尤其在科研和中小规模工业应用中表现突出：</p>
<h3 id="1-计算机视觉（Computer-Vision-CV）"><a href="#1-计算机视觉（Computer-Vision-CV）" class="headerlink" title="1. 计算机视觉（Computer Vision, CV）"></a>1. 计算机视觉（Computer Vision, CV）</h3><p>PyTorch的<code>torchvision</code>库是CV领域的核心工具，包含预训练模型、数据集、图像预处理函数，支持各类CV任务：</p>
<ul>
<li><strong>图像分类</strong>：用ResNet、EfficientNet、ViT（Vision Transformer）等模型做图像识别；</li>
<li><strong>目标检测</strong>：用Faster R-CNN、YOLOv5（第三方）、SSD等模型检测图像中的物体；</li>
<li><strong>图像分割</strong>：用U-Net、Mask R-CNN等模型做语义分割、实例分割；</li>
<li><strong>图像生成</strong>：用GAN（生成对抗网络）、StyleGAN、Diffusion Models（扩散模型）生成逼真图像。</li>
</ul>
<h4 id="示例：用预训练ResNet做图像分类"><a href="#示例：用预训练ResNet做图像分类" class="headerlink" title="示例：用预训练ResNet做图像分类"></a>示例：用预训练ResNet做图像分类</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models, transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载预训练ResNet50（ImageNet预训练权重）</span></span><br><span class="line">model = models.resnet50(pretrained=<span class="literal">True</span>)  <span class="comment"># 或用weights=models.ResNet50_Weights.IMAGENET1K_V1（PyTorch 2.x）</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 评估模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 图像预处理（与预训练模型一致）</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 加载并预处理图像</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&quot;cat.jpg&quot;</span>).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">image_tensor = transform(image).unsqueeze(<span class="number">0</span>)  <span class="comment"># 增加batch维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 推理</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 禁用梯度追踪，提升速度</span></span><br><span class="line">    outputs = model(image_tensor)</span><br><span class="line">    _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)  <span class="comment"># 获取预测类别索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 加载ImageNet类别标签并输出结果</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;imagenet_classes.txt&quot;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    classes = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;预测类别: <span class="subst">&#123;classes[predicted.item()]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>


<h3 id="2-自然语言处理（Natural-Language-Processing-NLP）"><a href="#2-自然语言处理（Natural-Language-Processing-NLP）" class="headerlink" title="2. 自然语言处理（Natural Language Processing, NLP）"></a>2. 自然语言处理（Natural Language Processing, NLP）</h3><p>PyTorch在NLP领域的优势显著，尤其是Hugging Face的<code>transformers</code>库（基于PyTorch）成为主流，支持各类NLP任务：</p>
<ul>
<li><strong>文本分类</strong>：情感分析、垃圾邮件检测；</li>
<li><strong>机器翻译</strong>：基于Transformer的中英互译；</li>
<li><strong>问答系统</strong>：抽取式问答（如BERT）、生成式问答（如GPT）；</li>
<li><strong>大语言模型（LLM）</strong>：训练&#x2F;微调GPT、LLaMA、ChatGLM等模型。</li>
</ul>
<h4 id="示例：用Hugging-Face-Transformers加载BERT做文本分类"><a href="#示例：用Hugging-Face-Transformers加载BERT做文本分类" class="headerlink" title="示例：用Hugging Face Transformers加载BERT做文本分类"></a>示例：用Hugging Face Transformers加载BERT做文本分类</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载预训练BERT模型和Tokenizer（情感分析任务）</span></span><br><span class="line">model_name = <span class="string">&quot;bert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line">model = BertForSequenceClassification.from_pretrained(model_name)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 预处理文本（Tokenize）</span></span><br><span class="line">text = <span class="string">&quot;I love PyTorch! It&#x27;s very easy to use.&quot;</span></span><br><span class="line">inputs = tokenizer(text, return_tensors=<span class="string">&quot;pt&quot;</span>, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 推理</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    logits = outputs.logits</span><br><span class="line">    predicted_class_id = torch.argmax(logits, dim=<span class="number">1</span>).item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 输出结果（0=负面，1=正面）</span></span><br><span class="line">class_names = [<span class="string">&quot;Negative&quot;</span>, <span class="string">&quot;Positive&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;文本情感: <span class="subst">&#123;class_names[predicted_class_id]&#125;</span>&quot;</span>)  <span class="comment"># 输出Positive</span></span><br></pre></td></tr></table></figure>


<h3 id="3-强化学习（Reinforcement-Learning-RL）"><a href="#3-强化学习（Reinforcement-Learning-RL）" class="headerlink" title="3. 强化学习（Reinforcement Learning, RL）"></a>3. 强化学习（Reinforcement Learning, RL）</h3><p>PyTorch的动态图特性非常适合强化学习（需频繁调整策略网络、价值网络），主流RL库（如Stable Baselines3、TorchRL）均基于PyTorch构建，支持：</p>
<ul>
<li><strong>游戏AI</strong>：Atari游戏、围棋、星际争霸；</li>
<li><strong>机器人控制</strong>：机械臂抓取、无人机导航；</li>
<li><strong>资源调度</strong>：数据中心GPU调度、交通流量优化。</li>
</ul>
<h3 id="4-科学计算与AI-科学"><a href="#4-科学计算与AI-科学" class="headerlink" title="4. 科学计算与AI+科学"></a>4. 科学计算与AI+科学</h3><p>PyTorch的张量运算和自动微分能力也被用于科学计算领域，如：</p>
<ul>
<li><strong>物理模拟</strong>：流体力学、量子力学模拟；</li>
<li><strong>生物医学</strong>：蛋白质结构预测（如AlphaFold 2基于PyTorch）；</li>
<li><strong>气候预测</strong>：基于深度学习的气象数据建模。</li>
</ul>
<h2 id="五、PyTorch的优缺点与适用人群"><a href="#五、PyTorch的优缺点与适用人群" class="headerlink" title="五、PyTorch的优缺点与适用人群"></a>五、PyTorch的优缺点与适用人群</h2><h3 id="1-优点"><a href="#1-优点" class="headerlink" title="1. 优点"></a>1. 优点</h3><ul>
<li><strong>动态图灵活性高</strong>：支持条件、循环、动态修改网络，调试直观（可直接<code>print</code>中间结果），科研友好；</li>
<li><strong>API简洁直观</strong>：接口设计贴近Python直觉，学习成本低，新手易上手；</li>
<li><strong>生态完善</strong>：TorchVision、Hugging Face Transformers、Stable Baselines3等扩展库覆盖全场景；</li>
<li><strong>科研社区活跃</strong>：学术界多数论文（如LLM、扩散模型）优先提供PyTorch实现，便于复现；</li>
<li><strong>性能持续提升</strong>：PyTorch 2.x的<code>torch.compile</code>大幅提升静态图性能，缩小与TensorFlow的差距。</li>
</ul>
<h3 id="2-缺点"><a href="#2-缺点" class="headerlink" title="2. 缺点"></a>2. 缺点</h3><ul>
<li><strong>大规模工业部署生态稍逊</strong>：虽然支持多平台部署，但在超大规模集群（如谷歌TPU集群）的优化上，仍不如TensorFlow成熟；</li>
<li><strong>早期版本兼容性问题</strong>：PyTorch 1.x到2.x的部分API有变化，旧代码需适配；</li>
<li><strong>多GPU训练配置复杂</strong>：DDP的配置（如分布式环境初始化）比TensorFlow的<code>tf.distribute</code>稍繁琐。</li>
</ul>
<h3 id="3-适用人群"><a href="#3-适用人群" class="headerlink" title="3. 适用人群"></a>3. 适用人群</h3><ul>
<li><strong>科研人员</strong>：需快速迭代算法、复现论文、构建复杂动态模型；</li>
<li><strong>初学者</strong>：希望低门槛入门深度学习，享受直观的调试体验；</li>
<li><strong>中小规模工业开发者</strong>：需快速开发产品原型，部署到移动端、嵌入式或小型云服务；</li>
<li><strong>不适用场景</strong>：超大规模工业部署（如谷歌、阿里的万亿级数据训练），TensorFlow更优。</li>
</ul>
<h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h2><p>PyTorch是一款“<strong>以灵活性和易用性为核心</strong>”的深度学习框架，其动态计算图设计彻底改变了深度学习的开发体验，尤其适配科研领域“快速迭代、复杂创新”的需求。随着PyTorch 2.x的发布，其在性能和部署能力上的短板不断弥补，形成了“科研→原型→产品”的全流程支持能力。</p>
<p>对于开发者而言，掌握PyTorch的关键是：</p>
<ol>
<li>理解核心概念（张量、动态计算图、自动微分）；</li>
<li>熟练使用<code>torch.nn</code>构建自定义模型，<code>torch.optim</code>优化参数；</li>
<li>用<code>torch.utils.data</code>构建高效数据流水线；</li>
<li>根据场景选择可视化（TensorBoard）和部署方式（TorchScript、ONNX）。</li>
</ol>
<p>PyTorch的成功不仅在于技术设计，更在于其“以开发者为中心”的理念——它让深度学习不再是“复杂的图定义游戏”，而是回归到“直观的代码逻辑”，这也是其在科研和开发者社区中持续流行的核心原因。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/09/22/%E5%B5%8C%E5%85%A5%E5%BC%8F/%E4%BB%80%E4%B9%88%E6%98%AFcpio/" rel="prev" title="深入理解cpio：Linux下的归档工具与嵌入式领域核心应用">
      <i class="fa fa-chevron-left"></i> 深入理解cpio：Linux下的归档工具与嵌入式领域核心应用
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/09/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/TensorFlow/" rel="next" title="TensorFlow深度学习框架：核心原理、组件、流程与实战详解">
      TensorFlow深度学习框架：核心原理、组件、流程与实战详解 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#title-PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%EF%BC%9A%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86%E3%80%81%E7%BB%84%E4%BB%B6%E3%80%81%E5%AE%9E%E6%88%98%E4%B8%8E%E7%94%9F%E6%80%81%E8%AF%A6%E8%A7%A3"><span class="nav-text">title: PyTorch深度学习框架：核心原理、组件、实战与生态详解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81PyTorch%E7%9A%84%E6%A0%B8%E5%BF%83%E5%AE%9A%E4%BD%8D%E4%B8%8E%E7%89%88%E6%9C%AC%E6%BC%94%E8%BF%9B"><span class="nav-text">一、PyTorch的核心定位与版本演进</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%A0%B8%E5%BF%83%E5%AE%9A%E4%BD%8D"><span class="nav-text">1. 核心定位</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%85%B3%E9%94%AE%E7%89%88%E6%9C%AC%E6%BC%94%E8%BF%9B"><span class="nav-text">2. 关键版本演进</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81PyTorch%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="nav-text">二、PyTorch的核心概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%BC%A0%E9%87%8F%EF%BC%88Tensor%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E7%9A%84%E8%BD%BD%E4%BD%93"><span class="nav-text">1. 张量（Tensor）：数据的载体</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E5%BC%A0%E9%87%8F%E7%9A%84%E6%A0%B8%E5%BF%83%E5%B1%9E%E6%80%A7"><span class="nav-text">（1）张量的核心属性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E6%93%8D%E4%BD%9C"><span class="nav-text">（2）张量的创建与操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%883%EF%BC%89%E5%8F%AF%E8%AE%AD%E7%BB%83%E5%BC%A0%E9%87%8F%EF%BC%9Arequires-grad%E4%B8%8E%E6%A2%AF%E5%BA%A6%E8%BF%BD%E8%B8%AA"><span class="nav-text">（3）可训练张量：requires_grad与梯度追踪</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%88Dynamic-Computation-Graph%EF%BC%89%EF%BC%9A%E7%81%B5%E6%B4%BB%E7%9A%84%E8%BF%90%E7%AE%97%E6%B5%81%E7%A8%8B"><span class="nav-text">2. 动态计算图（Dynamic Computation Graph）：灵活的运算流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%9B%BEvs%E9%9D%99%E6%80%81%E5%9B%BE%EF%BC%88%E4%BB%A5TensorFlow-1-x%E4%B8%BA%E4%BE%8B%EF%BC%89"><span class="nav-text">动态图vs静态图（以TensorFlow 1.x为例）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%9B%BE%E7%A4%BA%E4%BE%8B%EF%BC%9A%E5%B8%A6%E6%9D%A1%E4%BB%B6%E5%88%86%E6%94%AF%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="nav-text">动态图示例：带条件分支的模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%EF%BC%88Autograd%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97%E7%9A%84%E6%A0%B8%E5%BF%83"><span class="nav-text">3. 自动微分（Autograd）：梯度计算的核心</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E7%89%B9%E6%80%A7%EF%BC%9A"><span class="nav-text">关键特性：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81PyTorch%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%EF%BC%88%E6%9E%B6%E6%9E%84%E4%B8%8E%E7%94%9F%E6%80%81%EF%BC%89"><span class="nav-text">三、PyTorch的核心组件（架构与生态）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-torch-nn%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%84%E5%BB%BA%E6%A0%B8%E5%BF%83"><span class="nav-text">1. torch.nn：神经网络构建核心</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E6%A0%B8%E5%BF%83%E5%AE%B9%E5%99%A8%EF%BC%9ASequential%E4%B8%8EModule"><span class="nav-text">（1）核心容器：Sequential与Module</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B1%EF%BC%9A%E7%94%A8Sequential%E6%9E%84%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%EF%BC%88MNIST%E5%88%86%E7%B1%BB%EF%BC%89"><span class="nav-text">示例1：用Sequential构建全连接网络（MNIST分类）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B2%EF%BC%9A%E7%94%A8Module%E8%87%AA%E5%AE%9A%E4%B9%89%E6%AE%8B%E5%B7%AE%E5%9D%97%EF%BC%88ResNet%E6%A0%B8%E5%BF%83%EF%BC%89"><span class="nav-text">示例2：用Module自定义残差块（ResNet核心）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E5%B8%B8%E7%94%A8%E5%B1%82%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">（2）常用层与损失函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-torch-optim%EF%BC%9A%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0"><span class="nav-text">2. torch.optim：优化器与参数更新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E7%94%A8Adam%E4%BC%98%E5%8C%96%E5%99%A8%E8%AE%AD%E7%BB%83%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-text">示例：用Adam优化器训练线性模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%9A"><span class="nav-text">常用优化器：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-torch-utils-data%EF%BC%9A%E9%AB%98%E6%95%88%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E6%B5%81%E6%B0%B4%E7%BA%BF"><span class="nav-text">3. torch.utils.data：高效数据输入流水线</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B1%EF%BC%9A%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88%E5%8A%A0%E8%BD%BD%E6%9C%AC%E5%9C%B0%E5%9B%BE%E5%83%8F%EF%BC%89"><span class="nav-text">示例1：自定义图像数据集（加载本地图像）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B2%EF%BC%9A%E4%BD%BF%E7%94%A8%E5%86%85%E7%BD%AE%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88TorchVision%E6%8F%90%E4%BE%9B%EF%BC%89"><span class="nav-text">示例2：使用内置数据集（TorchVision提供）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7%EF%BC%9ATensorBoard%E4%B8%8EWeights-Biases"><span class="nav-text">4. 可视化工具：TensorBoard与Weights &amp; Biases</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89TensorBoard"><span class="nav-text">（1）TensorBoard</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89Weights-Biases%EF%BC%88W-B%EF%BC%89"><span class="nav-text">（2）Weights &amp; Biases（W&amp;B）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%EF%BC%9Atorch-distributed"><span class="nav-text">5. 分布式训练：torch.distributed</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E7%94%A8DDP%E5%AE%9E%E7%8E%B0%E5%8D%95%E6%9C%BA%E5%A4%9AGPU%E8%AE%AD%E7%BB%83"><span class="nav-text">示例：用DDP实现单机多GPU训练</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%EF%BC%9A%E4%BB%8E%E7%A7%91%E7%A0%94%E5%88%B0%E4%BA%A7%E5%93%81"><span class="nav-text">6. 模型部署：从科研到产品</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89TorchScript%EF%BC%9A%E5%8A%A8%E6%80%81%E5%9B%BE%E8%BD%AC%E9%9D%99%E6%80%81%E5%9B%BE"><span class="nav-text">（1）TorchScript：动态图转静态图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9ATorchScript%E8%BD%AC%E6%8D%A2%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="nav-text">示例：TorchScript转换与保存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89ONNX%EF%BC%9A%E8%B7%A8%E6%A1%86%E6%9E%B6%E8%BD%AC%E6%8D%A2"><span class="nav-text">（2）ONNX：跨框架转换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%883%EF%BC%89TorchServe%EF%BC%9A%E6%A8%A1%E5%9E%8B%E6%9C%8D%E5%8A%A1%E5%8C%96%E9%83%A8%E7%BD%B2"><span class="nav-text">（3）TorchServe：模型服务化部署</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%884%EF%BC%89TorchMobile%EF%BC%9A%E7%A7%BB%E5%8A%A8%E4%B8%8E%E5%B5%8C%E5%85%A5%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="nav-text">（4）TorchMobile：移动与嵌入式部署</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81PyTorch%E7%9A%84%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-text">四、PyTorch的典型应用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%EF%BC%88Computer-Vision-CV%EF%BC%89"><span class="nav-text">1. 计算机视觉（Computer Vision, CV）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83ResNet%E5%81%9A%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB"><span class="nav-text">示例：用预训练ResNet做图像分类</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88Natural-Language-Processing-NLP%EF%BC%89"><span class="nav-text">2. 自然语言处理（Natural Language Processing, NLP）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E7%94%A8Hugging-Face-Transformers%E5%8A%A0%E8%BD%BDBERT%E5%81%9A%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="nav-text">示例：用Hugging Face Transformers加载BERT做文本分类</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88Reinforcement-Learning-RL%EF%BC%89"><span class="nav-text">3. 强化学习（Reinforcement Learning, RL）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%8EAI-%E7%A7%91%E5%AD%A6"><span class="nav-text">4. 科学计算与AI+科学</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81PyTorch%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E4%B8%8E%E9%80%82%E7%94%A8%E4%BA%BA%E7%BE%A4"><span class="nav-text">五、PyTorch的优缺点与适用人群</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%BC%98%E7%82%B9"><span class="nav-text">1. 优点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%BC%BA%E7%82%B9"><span class="nav-text">2. 缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E9%80%82%E7%94%A8%E4%BA%BA%E7%BE%A4"><span class="nav-text">3. 适用人群</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81%E6%80%BB%E7%BB%93"><span class="nav-text">六、总结</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yxa"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">Yxa</p>
  <div class="site-description" itemprop="description">DO SOMETHING COOL！</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">95</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yanxianan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yanxianan" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yanxiaoan429@163.com" title="E-Mail → mailto:yanxiaoan429@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yxa</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
